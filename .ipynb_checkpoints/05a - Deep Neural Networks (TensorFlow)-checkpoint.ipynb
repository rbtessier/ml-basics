{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "# Deep Learning with Tensorflow\n",
        "\n",
        "Classical machine learning relies on using statistics to determine relationships between features and labels, and can be very effective for creating predictive models. However, a massive growth in the availability of data coupled with advances in the computing technology required to process it has led to the emergence of new machine learning techniques that mimic the way the brain processes information in a structure called an artificial neural network.\n",
        "\n",
        "## Creating a Neural Network with Tensorflow\n",
        "Tensorflow is a framework for creating machine learning models, including deep neural networks (DNNs). In this example, we'll use it to create a simple neural network that classifies iris flowers into species based on measurements of their petals and sepals.\n",
        "\n",
        "> The iris classification model is a very common machine learning example, and the iris dataset is often the basis for \"hello world\" sample code for a wide range of machine learning frameworks. In reality, you can solve this problem easily using classical machine learning techniques without the need for a deep learning model; but it's a useful, easy to understand dataset with which to demonstrate the principles of neural networks in this notebook.\n",
        "\n",
        "### Exploring the Iris Dataset\n",
        "Before we start using Tensorflow to create a model, let's examine the iris dataset. Since this is a commonly used sample dataset, it is built-in to the *scikit-learn* machine learning library, so we'll get it from there. As with any supervised learning problem, we'll then split the dataset into a set of records with which to train the model, and a smaller set with which to validate the trained model."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Split data 60%-40% into training set and test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.40, random_state=0)\n",
        "\n",
        "print ('Training Set: %d, Test Set: %d \\n' % (len(x_train), len(x_test)))\n",
        "print(\"Sample of features and labels:\")\n",
        "print('(features: ',iris.feature_names, ')')\n",
        "\n",
        "# Take a look at the first 25 training features and corresponding labels\n",
        "for n in range(0,24):\n",
        "    print(x_train[n], y_train[n], '(' + iris.target_names[y_train[n]] + ')')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "The *features* are the measurements for each iris observation, and the *label* is a numeric value that indicates the species of iris that the observation represents (versicolor, virginica, or setosa).\n",
        "\n",
        "### Import Tensorflow Libraries\n",
        "Since we plan to use Tensorflow to create our iris classifier, we'll need to install and import the libraries we intend to use. Tensorflow is already installed in your environment, so we just need to import the libraries we need."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "print(\"Libraries imported.\")\n",
        "print('Keras version:',keras.__version__)\n",
        "print('TensorFlow version:',tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Prepare the Data for Tensorflow\n",
        "We've already loaded our data and split it into training and validation datasets. However, we need to do some further data preparation so that our data will work correctly with Tensorflow. Specifically, we need to set the data type of our features to 32-bit floating point numbers, and specify that the labels represent categorical classes rather than numeric values."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Set data types for float features\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Set data types for categorical labels\n",
        "y_train = utils.to_categorical(y_train)\n",
        "y_test = utils.to_categorical(y_test)\n",
        "print('Ready...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Define a Neural Network\n",
        "Now we're ready to define our neural network. In this case, we'll create a network that consists of 3 fully-connected layers:\n",
        "* An input layer that receives four input values (the iris features) and applies a *ReLU* activation function to produce ten outputs.\n",
        "* A hidden layer that receives ten inputs and applies a *ReLU* activation function to produce another ten outputs.\n",
        "* An output layer that uses a *SoftMax* activation function to generate three outputs (which represent the probabilities for the three iris species)"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Define a classifier network\n",
        "hl = 10 # Number of hidden layer nodes\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(hl, input_dim=4, activation='relu'))\n",
        "model.add(Dense(hl, input_dim=hl, activation='relu'))\n",
        "model.add(Dense(3, input_dim=3, activation='softmax'))\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Train the Model\n",
        "To train the model, we need to repeatedly feed the training values forward through the network, use a loss function to calculate the loss, use an optimizer to backpropagate the weight and bias value adjustments, and validate the model using the test data we withheld.\n",
        "\n",
        "To do this, we'll apply a Stochastic Gradient Descent optimizer to a categorical cross-entropy loss function iteratively over 50 epochs."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": [
        "#hyper-parameters for optimizer\n",
        "learning_rate = 0.01\n",
        "learning_momentum = 0.9\n",
        "sgd = optimizers.SGD(lr=learning_rate, momentum = learning_momentum)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model over 50 epochs using 10-observation batches and using the test holdout dataset for validation\n",
        "num_epochs = 50\n",
        "history = model.fit(x_train, y_train, epochs=num_epochs, batch_size=10, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Review Training and Validation Loss\n",
        "After training is complete, we can examine the loss metrics we recorded while training and validating the model. We're really looking for two things:\n",
        "* The loss should reduce with each epoch, showing that the model is learning the right weights and biases to predict the correct labels.\n",
        "* The training loss and validations loss should follow a similar trend, showing that the model is not overfitting to the training data.\n",
        "\n",
        "Let's plot the loss metrics and see:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "epoch_nums = range(1,num_epochs+1)\n",
        "training_loss = history.history[\"loss\"]\n",
        "validation_loss = history.history[\"val_loss\"]\n",
        "plt.plot(epoch_nums, training_loss)\n",
        "plt.plot(epoch_nums, validation_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['training', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### View the Learned Weights and Biases\n",
        "The trained model consists of the final weights and biases that were determined by the optimizer during training. Based on our network model we should expect the following values for each layer:\n",
        "* Layer 1: There are four input values going to ten output nodes, so there should be 10 x 4 weights and 10 bias values.\n",
        "* Layer 2: There are ten input values going to ten output nodes, so there should be 10 x 10 weights and 10 bias values.\n",
        "* Layer 3: There are ten input values going to three output nodes, so there should be 10 x 3 weights and 3 bias values."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "for layer in model.layers:\n",
        "    weights = layer.get_weights()[0]\n",
        "    biases = layer.get_weights()[1]\n",
        "    print('------------\\nWeights:\\n',weights,'\\nBiases:\\n', biases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model Performance\n",
        "So, is the model any good? The raw accuracy reported from the validation data would seem to indicate that it predicts pretty well; but it's typically useful to dig a little deeper and compare the predictions for each possible class. A common way to visualize the performace of a classification model is to create a *confusion matrix* that shows a crosstab of correct and incorrect predictions for each class."
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Tensorflow doesn't have a built-in confusion matrix metric, so we'll use SciKit-Learn\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "class_probabilities = model.predict(x_test)\n",
        "predictions = np.argmax(class_probabilities, axis=1)\n",
        "true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(iris.target_names))\n",
        "plt.xticks(tick_marks, iris.target_names, rotation=85)\n",
        "plt.yticks(tick_marks, iris.target_names)\n",
        "plt.xlabel(\"Predicted Class\")\n",
        "plt.ylabel(\"True Class\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "The confusion matrix should show a strong diagonal line indicating that there are more correct than incorrect predictions for each class.\n",
        "\n",
        "### Using the Model with New Data\n",
        "Now that we have a model we believe is reasonably accurate, we can use it to predict the species of new iris observations:"
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "modelFileName = 'models/iris-classifier.h5'\n",
        "model.save(modelFileName)\n",
        "\n",
        "# Load the saved model when required\n",
        "del model  # deletes the existing model variable\n",
        "model = models.load_model(modelFileName) # loads the saved model\n",
        "\n",
        "x_new = np.array([[6.6,3.2,5.8,2.4]])\n",
        "print ('New sample: {}'.format(x_new))\n",
        "\n",
        "class_probabilities = model.predict(x_new)\n",
        "predictions = np.argmax(class_probabilities, axis=1)\n",
        "\n",
        "print(iris.target_names[predictions][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": [
        "## Learn More\n",
        "This notebook was designed to help you understand the basic concepts and principles involved in deep neural networks, using a simple Tensorflow example. To learn more about Tensorflow, take a look at the <a href=\"https://www.tensorflow.org/\" target=\"_blank\">Tensorflow web site</a>."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}